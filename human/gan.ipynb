{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HHwSL_PIYHK0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MutianWang/novel-cell/blob/main/human/gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5IPyySLKxff"
      },
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAf_GqpbxTMO",
        "outputId": "742ff5d6-35a2-4106-d7b6-811896dd9f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-IQa1DUO3wv"
      },
      "source": [
        "path = '/content/drive/My Drive/Colab Notebooks/Brain Cell/data/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbdI7rTr1LHf"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvqF5fu3QPLd"
      },
      "source": [
        "meta = pd.read_csv(path+'meta.csv', header=0)\n",
        "cols_glut = meta[meta['class']=='Glutamatergic']['sample_name']\n",
        "cols_non = meta[meta['class']=='Non-neuronal']['sample_name']\n",
        "cols_gaba = meta[meta['class']=='GABAergic']['sample_name']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eP4_BjYw9wZ"
      },
      "source": [
        "def normalize(df):\n",
        "    # counts per million\n",
        "    df = df.div(df.sum(axis=1), axis=0) * 10**6\n",
        "    df = df.fillna(0)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5auoM8d0Pob"
      },
      "source": [
        "def read_expression(file1, file2, usecols):\n",
        "    # point-wise addition of exon and intron tables\n",
        "    reader1 = pd.read_csv(file1, header=0, usecols=usecols, chunksize=1000)\n",
        "    reader2 = pd.read_csv(file2, header=0, usecols=usecols, chunksize=1000)\n",
        "\n",
        "    df = normalize(reader1.get_chunk() + reader2.get_chunk())\n",
        "    for i in range(1, 51):\n",
        "        df = pd.concat([df, normalize(reader1.get_chunk() + reader2.get_chunk())])\n",
        "        if i%10==0:\n",
        "            print('{}/50'.format(i))\n",
        "\n",
        "    return df.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GORuOJYCXjY",
        "outputId": "ee24234a-d502-4e5d-9224-82c4e886e3d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "exp_glut = read_expression(path+'exon.csv', path+'intron.csv', cols_glut) # 10525 * 50281\n",
        "exp_non = read_expression(path+'exon.csv', path+'intron.csv', cols_non) # 914 * 50281\n",
        "exp_gaba = read_expression(path+'exon.csv', path+'intron.csv', cols_gaba) # 4164 * 50281"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/50\n",
            "20/50\n",
            "30/50\n",
            "40/50\n",
            "50/50\n",
            "10/50\n",
            "20/50\n",
            "30/50\n",
            "40/50\n",
            "50/50\n",
            "10/50\n",
            "20/50\n",
            "30/50\n",
            "40/50\n",
            "50/50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYNolVf-JUvm"
      },
      "source": [
        "np.save(path+'exp_glut', exp_glut)\n",
        "np.save(path+'exp_non', exp_non)\n",
        "np.save(path+'exp_gaba', exp_gaba)\n",
        "\n",
        "# clear RAM\n",
        "del exp_glut, exp_non, exp_gaba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-F04FN3Sh0v"
      },
      "source": [
        "exp_glut = np.load(path+'exp_glut.npy')[:5000-914]\n",
        "exp_non = np.load(path+'exp_non.npy')\n",
        "\n",
        "# first 4086 are Glutamatergic, last 914 are Non-neuronal\n",
        "exp_train = np.concatenate([exp_glut, exp_non], axis=0)\n",
        "\n",
        "# clear RAM\n",
        "del exp_glut, exp_non"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7FLNEy-UAd0"
      },
      "source": [
        "np.save(path+'exp_train', exp_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4ZHqOU3AAiL"
      },
      "source": [
        "## Dimension Reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC6DTCZ5ACMy"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8NeA1FwKh-v"
      },
      "source": [
        "pipe = Pipeline([('scaler1', StandardScaler()), ('pca', PCA(n_components=5000)), ('scaler2', MinMaxScaler())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDtxSFLtAVM-"
      },
      "source": [
        "#exp_train = np.load(path+'exp_train.npy')\n",
        "exp_train = pipe.fit_transform(exp_train)\n",
        "np.save(path+'exp_train_pca', exp_train)\n",
        "del exp_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-3XFhfC0Bm-"
      },
      "source": [
        "exp_test = np.load(path+'exp_gaba.npy')\n",
        "exp_test = pipe.transform(exp_test)\n",
        "np.save(path+'exp_test_pca', exp_test)\n",
        "del exp_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsH4XxJ36fGp"
      },
      "source": [
        "## GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7WRBp4jyxPS"
      },
      "source": [
        "exp_train = np.load(path+'exp_train_pca.npy')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKjKbveG8xT3"
      },
      "source": [
        "dimension = exp_train.shape[1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0jLivwZJyzt"
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hYhAVII5tVz"
      },
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(256, use_bias=True, input_shape=(128,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Dense(512, use_bias=True))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Dense(512, use_bias=True))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Dense(1024, use_bias=True))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Dense(2048, use_bias=True))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # sigmoid function will make the range [0,1]\n",
        "    model.add(layers.Dense(dimension, use_bias=True, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmuNxZCu8o0p"
      },
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(1024, use_bias=True, input_shape=(dimension,)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(512, use_bias=True))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(256, use_bias=True))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6HmejF9-U6r"
      },
      "source": [
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFxhmXU0Lak3",
        "outputId": "a4f94706-1a8a-46f8-83de-3acba904a3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "generator.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2048)              2099200   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 2048)              8192      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5000)              10245000  \n",
            "=================================================================\n",
            "Total params: 13,314,184\n",
            "Trainable params: 13,305,480\n",
            "Non-trainable params: 8,704\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LDI3iJBLdTD",
        "outputId": "569f57bd-d7a1-40ea-ac5b-b7d42eacc713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "discriminator.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 1024)              5121024   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 5,777,409\n",
            "Trainable params: 5,777,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmTf7v9p-b50"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_orWQwj_Ak-"
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llUBEF24_BJS"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBlm7JMx_DvY"
      },
      "source": [
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, 128])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNntEwY__OTQ"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "\n",
        "        for data in dataset:\n",
        "            train_step(data)\n",
        "\n",
        "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L42hxrlDHmb4"
      },
      "source": [
        "BATCH_SIZE = 500\n",
        "dataset = tf.data.Dataset.from_tensor_slices(exp_train).shuffle(1000).batch(BATCH_SIZE)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-axc-FmH8Dn",
        "outputId": "8e32940f-09f2-48b4-a936-63318ca0dcad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "train(dataset, 10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 1 is 3.2583796977996826 sec\n",
            "Time for epoch 2 is 1.5587396621704102 sec\n",
            "Time for epoch 3 is 1.5188868045806885 sec\n",
            "Time for epoch 4 is 1.5487749576568604 sec\n",
            "Time for epoch 5 is 1.4790289402008057 sec\n",
            "Time for epoch 6 is 1.4789140224456787 sec\n",
            "Time for epoch 7 is 1.4853599071502686 sec\n",
            "Time for epoch 8 is 1.4999761581420898 sec\n",
            "Time for epoch 9 is 1.475475788116455 sec\n",
            "Time for epoch 10 is 1.4388682842254639 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFt8CKk7H9U4"
      },
      "source": [
        "exp_gen = generator(tf.random.normal([1000,128]))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fSZcalQ95M1"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3d-0ISkWvSP"
      },
      "source": [
        "### mean center"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcPOAm4dKoF9"
      },
      "source": [
        "from scipy.spatial import distance"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_ABejoI94h2"
      },
      "source": [
        "mean_glut = np.mean(np.load(path+'exp_train_pca.npy')[:4086], axis=0)\n",
        "mean_non = np.mean(np.load(path+'exp_train_pca.npy')[4086:], axis=0)\n",
        "mean_gaba = np.mean(np.load(path+'exp_test_pca.npy'), axis=0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACzAWnruVv_T",
        "outputId": "70757c85-4f88-4a2c-b73c-01b3807ffd2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# last value of mean_gaba is too small, so it's excluded when computing the distance\n",
        "# this approach is identical to reducing the dimension to 4999\n",
        "mean_gaba[-1]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.597220239451883e+22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgfnJ1wjjyeH",
        "outputId": "3a2eb860-6267-4481-d11a-9537f30d965d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# L1 distance\n",
        "res = [0, 0, 0]\n",
        "tmp = []\n",
        "for exp in exp_gen:\n",
        "    d1 = np.sum(np.abs(exp-mean_glut)[:-1])\n",
        "    d2 = np.sum(np.abs(exp-mean_non)[:-1])\n",
        "    d3 = np.sum(np.abs(exp-mean_gaba)[:-1])\n",
        "    tmp.append(d2)\n",
        "    i = np.argmin([d1, d2, d3])\n",
        "    res[i] += 1\n",
        "\n",
        "assert sum(res) == exp_gen.shape[0]\n",
        "\n",
        "print(res)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[734, 266, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LACUYIKwfad1",
        "outputId": "7e4405bf-98c7-49de-81d3-22f6b38c0f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# L2 distance\n",
        "res = [0, 0, 0]\n",
        "for i, exp in enumerate(exp_gen):\n",
        "    d1 = np.sum(np.square(exp-mean_glut)[:-1])\n",
        "    d2 = np.sum(np.square(exp-mean_non)[:-1])\n",
        "    d3 = np.sum(np.square(exp-mean_gaba)[:-1])\n",
        "    i = np.argmin([d1, d2, d3])\n",
        "    res[i] += 1\n",
        "\n",
        "assert sum(res) == exp_gen.shape[0]\n",
        "\n",
        "print(res)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in square\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[733, 267, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h6jO8gSIlT_",
        "outputId": "5f9ea440-8430-4e8a-8729-0e169b7d18db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# cosine distance\n",
        "res = [0, 0, 0]\n",
        "for exp in exp_gen:\n",
        "    d1 = distance.cosine(exp[:-1], mean_glut[:-1])\n",
        "    d2 = distance.cosine(exp[:-1], mean_non[:-1])\n",
        "    d3 = distance.cosine(exp[:-1], mean_gaba[:-1])\n",
        "    i = np.argmin([d1, d2, d3])\n",
        "    res[i] += 1\n",
        "\n",
        "assert sum(res) == exp_gen.shape[0]\n",
        "\n",
        "print(res)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[748, 252, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzUB_c8DWy42"
      },
      "source": [
        "### median center"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH-zh77AW3SL"
      },
      "source": [
        "median_glut = np.median(np.load(path+'exp_train_pca.npy')[:4086], axis=0)\n",
        "median_non = np.median(np.load(path+'exp_train_pca.npy')[4086:], axis=0)\n",
        "median_gaba = np.median(np.load(path+'exp_test_pca.npy'), axis=0)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4MiQLwmW3ST",
        "outputId": "663d7b63-d1fe-4748-bff9-395aad0ad06f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# last value of mean_gaba is too small, so it's excluded when computing the distance\n",
        "median_gaba[-1]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2.923588706429223e+21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gCQ74IcW3SX",
        "outputId": "e6d616b8-086c-42e7-f107-9009611af8e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# L1 distance\n",
        "res = [0, 0, 0]\n",
        "tmp = []\n",
        "for exp in exp_gen:\n",
        "    d1 = np.sum(np.abs(exp-median_glut)[:-1])\n",
        "    d2 = np.sum(np.abs(exp-median_non)[:-1])\n",
        "    d3 = np.sum(np.abs(exp-median_gaba)[:-1])\n",
        "    tmp.append(d2)\n",
        "    i = np.argmin([d1, d2, d3])\n",
        "    res[i] += 1\n",
        "\n",
        "assert sum(res) == exp_gen.shape[0]\n",
        "\n",
        "print(res)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[731, 269, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plagJGHCW3SZ",
        "outputId": "67135232-eaec-47fd-8a63-93cd883f9f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# L2 distance\n",
        "res = [0, 0, 0]\n",
        "for i, exp in enumerate(exp_gen):\n",
        "    d1 = np.sum(np.square(exp-median_glut)[:-1])\n",
        "    d2 = np.sum(np.square(exp-median_non)[:-1])\n",
        "    d3 = np.sum(np.square(exp-median_gaba)[:-1])\n",
        "    i = np.argmin([d1, d2, d3])\n",
        "    res[i] += 1\n",
        "\n",
        "assert sum(res) == exp_gen.shape[0]\n",
        "\n",
        "print(res)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in square\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[731, 269, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkeDMxgRW3Sb",
        "outputId": "90d1f27b-37b0-488a-e169-91c4f8710dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# cosine distance\n",
        "res = [0, 0, 0]\n",
        "for exp in exp_gen:\n",
        "    d1 = distance.cosine(exp[:-1], median_glut[:-1])\n",
        "    d2 = distance.cosine(exp[:-1], median_non[:-1])\n",
        "    d3 = distance.cosine(exp[:-1], median_gaba[:-1])\n",
        "    i = np.argmin([d1, d2, d3])\n",
        "    res[i] += 1\n",
        "\n",
        "assert sum(res) == exp_gen.shape[0]\n",
        "\n",
        "print(res)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[740, 257, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r7C0fF_Xl2c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}